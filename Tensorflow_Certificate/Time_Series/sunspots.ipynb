{"cells":[{"cell_type":"markdown","metadata":{"id":"kXNdAuSzoDHO"},"source":["<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/main/C4/W4/ungraded_labs/C4_W4_Lab_3_Sunspots_CNN_RNN_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Ywktc8UkWbRv"},"source":["# Ungraded Lab: Predicting Sunspots with Neural Networks\n","\n","At this point in the course, you should be able to explore different network architectures for forecasting. In the previous weeks, you've used DNNs, RNNs, and CNNs to build these different models. In the final practice lab for this course, you'll try one more configuration and that is a combination of all these types of networks: the data windows will pass through a convolution, followed by stacked LSTMs, followed by stacked dense layers. See if this improves results or you can just opt for simpler models."]},{"cell_type":"markdown","metadata":{"id":"JR3s1iNXW8qv"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"sLl52leVp5wU","executionInfo":{"status":"ok","timestamp":1673534288155,"user_tz":300,"elapsed":2610,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import csv"]},{"cell_type":"markdown","metadata":{"id":"UTgnwTJiW-bF"},"source":["## Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDrgVqCTao9j"},"outputs":[],"source":["def plot_series(x, y, format=\"-\", start=0, end=None, \n","                title=None, xlabel=None, ylabel=None, legend=None ):\n","    \"\"\"\n","    Visualizes time series data\n","\n","    Args:\n","      x (array of int) - contains values for the x-axis\n","      y (array of int or tuple of arrays) - contains the values for the y-axis\n","      format (string) - line style when plotting the graph\n","      start (int) - first time step to plot\n","      end (int) - last time step to plot\n","      title (string) - title of the plot\n","      xlabel (string) - label for the x-axis\n","      ylabel (string) - label for the y-axis\n","      legend (list of strings) - legend for the plot\n","    \"\"\"\n","\n","    # Setup dimensions of the graph figure\n","    plt.figure(figsize=(10, 6))\n","    \n","    # Check if there are more than two series to plot\n","    if type(y) is tuple:\n","\n","      # Loop over the y elements\n","      for y_curr in y:\n","\n","        # Plot the x and current y values\n","        plt.plot(x[start:end], y_curr[start:end], format)\n","\n","    else:\n","      # Plot the x and y values\n","      plt.plot(x[start:end], y[start:end], format)\n","\n","    # Label the x-axis\n","    plt.xlabel(xlabel)\n","\n","    # Label the y-axis\n","    plt.ylabel(ylabel)\n","\n","    # Set the legend\n","    if legend:\n","      plt.legend(legend)\n","\n","    # Set the title\n","    plt.title(title)\n","\n","    # Overlay a grid on the graph\n","    plt.grid(True)\n","\n","    # Draw the graph on screen\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"AU9FU6-BXE56"},"source":["## Download and Preview the Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"miE0cEqdu3eS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673534288340,"user_tz":300,"elapsed":190,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"be3afb26-0a47-4bc6-d84c-ba1e9230cb70"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-01-12 14:38:07--  https://storage.googleapis.com/tensorflow-1-public/course4/Sunspots.csv\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 173.194.202.128, 74.125.20.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 70827 (69K) [text/csv]\n","Saving to: ‘Sunspots.csv’\n","\n","\rSunspots.csv          0%[                    ]       0  --.-KB/s               \rSunspots.csv        100%[===================>]  69.17K  --.-KB/s    in 0.001s  \n","\n","2023-01-12 14:38:08 (91.4 MB/s) - ‘Sunspots.csv’ saved [70827/70827]\n","\n"]}],"source":["# Download the Dataset\n","!wget https://storage.googleapis.com/tensorflow-1-public/course4/Sunspots.csv"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NcG9r1eClbTh","executionInfo":{"status":"ok","timestamp":1673534291368,"user_tz":300,"elapsed":285,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}}},"outputs":[],"source":["# Initialize lists\n","time_step = []\n","sunspots = []\n","\n","# Open CSV file\n","with open('./Sunspots.csv') as csvfile:\n","  \n","  # Initialize reader\n","  reader = csv.reader(csvfile, delimiter=',')\n","  \n","  # Skip the first line\n","  next(reader)\n","  \n","  # Append row and sunspot number to lists\n","  for row in reader:\n","    time_step.append(int(row[0]))\n","    sunspots.append(float(row[2]))\n","\n","# Convert lists to numpy arrays\n","time = np.array(time_step)\n","series = np.array(sunspots)\n","\n","# Preview the data\n","# plot_series(time, series, xlabel='Month', ylabel='Monthly Mean Total Sunspot Number')"]},{"cell_type":"markdown","metadata":{"id":"Tq34m86mXHk4"},"source":["## Split the Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"L92YRw_IpCFG","executionInfo":{"status":"ok","timestamp":1673534373830,"user_tz":300,"elapsed":185,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}}},"outputs":[],"source":["# Define the split time\n","split_time = 3000\n","\n","# Get the train set \n","time_train = time[:split_time]\n","x_train = series[:split_time]\n","\n","# Get the validation set\n","time_valid = time[split_time:]\n","x_valid = series[split_time:]"]},{"cell_type":"markdown","metadata":{"id":"jlJDtNMDXkJF"},"source":["## Prepare Features and Labels"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lJwUUZscnG38","executionInfo":{"status":"ok","timestamp":1673534375646,"user_tz":300,"elapsed":161,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}}},"outputs":[],"source":["def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n","    \"\"\"Generates dataset windows\n","\n","    Args:\n","      series (array of float) - contains the values of the time series\n","      window_size (int) - the number of time steps to include in the feature\n","      batch_size (int) - the batch size\n","      shuffle_buffer(int) - buffer size to use for the shuffle method\n","\n","    Returns:\n","      dataset (TF Dataset) - TF Dataset containing time windows\n","    \"\"\"\n","  \n","    # Generate a TF Dataset from the series values\n","    dataset = tf.data.Dataset.from_tensor_slices(series)\n","    \n","    # Window the data but only take those with the specified size\n","    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n","    \n","    # Flatten the windows by putting its elements in a single batch\n","    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n","\n","    # Create tuples with features and labels \n","    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n","\n","    # Shuffle the windows\n","    dataset = dataset.shuffle(shuffle_buffer)\n","    \n","    # Create batches of windows\n","    dataset = dataset.batch(batch_size).prefetch(1)\n","    \n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"lakVlpDry5iv"},"source":["As mentioned in the lectures, if your results don't good, you can try tweaking the parameters here and see if the model will learn better."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"1Z0qRGp1zl5b","executionInfo":{"status":"ok","timestamp":1673534380769,"user_tz":300,"elapsed":2951,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}}},"outputs":[],"source":["# Parameters\n","window_size = 30\n","batch_size = 32\n","shuffle_buffer_size = 1000\n","\n","# Generate the dataset windows\n","train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"]},{"cell_type":"markdown","metadata":{"id":"ylQoAXTaXscV"},"source":["## Build the Model\n","\n","You've seen these layers before and here is how it's looks like when combined."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"y6kJd40-0Hj9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673534384205,"user_tz":300,"elapsed":1449,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"ba515589-6adb-47e8-d448-6f1af65646ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 30, 64)            256       \n","                                                                 \n"," lstm (LSTM)                 (None, 30, 64)            33024     \n","                                                                 \n"," lstm_1 (LSTM)               (None, 64)                33024     \n","                                                                 \n"," dense (Dense)               (None, 30)                1950      \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                310       \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 11        \n","                                                                 \n"," lambda (Lambda)             (None, 1)                 0         \n","                                                                 \n","=================================================================\n","Total params: 68,575\n","Trainable params: 68,575\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Build the Model\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n","                      strides=1,\n","                      activation=\"relu\",\n","                      padding='causal',\n","                      input_shape=[window_size, 1]),\n","  tf.keras.layers.LSTM(64, return_sequences=True),\n","  tf.keras.layers.LSTM(64),\n","  tf.keras.layers.Dense(30, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"relu\"),\n","  tf.keras.layers.Dense(1),\n","  tf.keras.layers.Lambda(lambda x: x * 400)\n","])\n","\n"," # Print the model summary \n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"EtSCmjQfXyKi"},"source":["## Tune the Learning Rate\n","\n","As usual, you will want to pick an optimal learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpVZbobXM117"},"outputs":[],"source":["# Get initial weights\n","init_weights = model.get_weights()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AclfYY3Mn6Ph","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673370243026,"user_tz":300,"elapsed":115660,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"db9a8143-4bb9-48a3-8ae6-99e6f93ae9ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","93/93 [==============================] - 12s 13ms/step - loss: 71.9792 - lr: 1.0000e-08\n","Epoch 2/100\n","93/93 [==============================] - 1s 9ms/step - loss: 57.9914 - lr: 1.1220e-08\n","Epoch 3/100\n","93/93 [==============================] - 1s 10ms/step - loss: 49.9074 - lr: 1.2589e-08\n","Epoch 4/100\n","93/93 [==============================] - 2s 15ms/step - loss: 43.8977 - lr: 1.4125e-08\n","Epoch 5/100\n","93/93 [==============================] - 2s 14ms/step - loss: 38.4898 - lr: 1.5849e-08\n","Epoch 6/100\n","93/93 [==============================] - 1s 9ms/step - loss: 34.9113 - lr: 1.7783e-08\n","Epoch 7/100\n","93/93 [==============================] - 1s 9ms/step - loss: 32.2641 - lr: 1.9953e-08\n","Epoch 8/100\n","93/93 [==============================] - 1s 9ms/step - loss: 30.2452 - lr: 2.2387e-08\n","Epoch 9/100\n","93/93 [==============================] - 1s 10ms/step - loss: 28.5937 - lr: 2.5119e-08\n","Epoch 10/100\n","93/93 [==============================] - 1s 10ms/step - loss: 27.2523 - lr: 2.8184e-08\n","Epoch 11/100\n","93/93 [==============================] - 1s 10ms/step - loss: 26.0899 - lr: 3.1623e-08\n","Epoch 12/100\n","93/93 [==============================] - 1s 10ms/step - loss: 25.3485 - lr: 3.5481e-08\n","Epoch 13/100\n","93/93 [==============================] - 1s 10ms/step - loss: 24.6705 - lr: 3.9811e-08\n","Epoch 14/100\n","93/93 [==============================] - 1s 10ms/step - loss: 24.1200 - lr: 4.4668e-08\n","Epoch 15/100\n","93/93 [==============================] - 1s 10ms/step - loss: 23.6741 - lr: 5.0119e-08\n","Epoch 16/100\n","93/93 [==============================] - 1s 14ms/step - loss: 23.2379 - lr: 5.6234e-08\n","Epoch 17/100\n","93/93 [==============================] - 1s 10ms/step - loss: 22.8624 - lr: 6.3096e-08\n","Epoch 18/100\n","93/93 [==============================] - 1s 9ms/step - loss: 22.5519 - lr: 7.0795e-08\n","Epoch 19/100\n","93/93 [==============================] - 1s 10ms/step - loss: 22.2551 - lr: 7.9433e-08\n","Epoch 20/100\n","93/93 [==============================] - 1s 9ms/step - loss: 21.9461 - lr: 8.9125e-08\n","Epoch 21/100\n","93/93 [==============================] - 1s 10ms/step - loss: 21.5837 - lr: 1.0000e-07\n","Epoch 22/100\n","93/93 [==============================] - 1s 9ms/step - loss: 21.2697 - lr: 1.1220e-07\n","Epoch 23/100\n","93/93 [==============================] - 1s 10ms/step - loss: 21.0785 - lr: 1.2589e-07\n","Epoch 24/100\n","93/93 [==============================] - 1s 9ms/step - loss: 20.9780 - lr: 1.4125e-07\n","Epoch 25/100\n","93/93 [==============================] - 1s 13ms/step - loss: 20.7951 - lr: 1.5849e-07\n","Epoch 26/100\n","93/93 [==============================] - 1s 13ms/step - loss: 20.6208 - lr: 1.7783e-07\n","Epoch 27/100\n","93/93 [==============================] - 1s 9ms/step - loss: 20.2939 - lr: 1.9953e-07\n","Epoch 28/100\n","93/93 [==============================] - 1s 9ms/step - loss: 20.0944 - lr: 2.2387e-07\n","Epoch 29/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.9515 - lr: 2.5119e-07\n","Epoch 30/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.8281 - lr: 2.8184e-07\n","Epoch 31/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.6689 - lr: 3.1623e-07\n","Epoch 32/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.7026 - lr: 3.5481e-07\n","Epoch 33/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.2881 - lr: 3.9811e-07\n","Epoch 34/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.3604 - lr: 4.4668e-07\n","Epoch 35/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.1365 - lr: 5.0119e-07\n","Epoch 36/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.2859 - lr: 5.6234e-07\n","Epoch 37/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.1456 - lr: 6.3096e-07\n","Epoch 38/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.6687 - lr: 7.0795e-07\n","Epoch 39/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.4567 - lr: 7.9433e-07\n","Epoch 40/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.7204 - lr: 8.9125e-07\n","Epoch 41/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.5689 - lr: 1.0000e-06\n","Epoch 42/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.6152 - lr: 1.1220e-06\n","Epoch 43/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.8810 - lr: 1.2589e-06\n","Epoch 44/100\n","93/93 [==============================] - 1s 9ms/step - loss: 18.9062 - lr: 1.4125e-06\n","Epoch 45/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.6957 - lr: 1.5849e-06\n","Epoch 46/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.5028 - lr: 1.7783e-06\n","Epoch 47/100\n","93/93 [==============================] - 1s 9ms/step - loss: 18.6478 - lr: 1.9953e-06\n","Epoch 48/100\n","93/93 [==============================] - 1s 9ms/step - loss: 18.8246 - lr: 2.2387e-06\n","Epoch 49/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.0037 - lr: 2.5119e-06\n","Epoch 50/100\n","93/93 [==============================] - 1s 9ms/step - loss: 18.3928 - lr: 2.8184e-06\n","Epoch 51/100\n","93/93 [==============================] - 1s 9ms/step - loss: 18.9909 - lr: 3.1623e-06\n","Epoch 52/100\n","93/93 [==============================] - 1s 9ms/step - loss: 18.4371 - lr: 3.5481e-06\n","Epoch 53/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.4220 - lr: 3.9811e-06\n","Epoch 54/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.3685 - lr: 4.4668e-06\n","Epoch 55/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.7183 - lr: 5.0119e-06\n","Epoch 56/100\n","93/93 [==============================] - 1s 9ms/step - loss: 18.8366 - lr: 5.6234e-06\n","Epoch 57/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.0748 - lr: 6.3096e-06\n","Epoch 58/100\n","93/93 [==============================] - 1s 9ms/step - loss: 20.3224 - lr: 7.0795e-06\n","Epoch 59/100\n","93/93 [==============================] - 1s 9ms/step - loss: 21.1957 - lr: 7.9433e-06\n","Epoch 60/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.9866 - lr: 8.9125e-06\n","Epoch 61/100\n","93/93 [==============================] - 1s 9ms/step - loss: 20.6180 - lr: 1.0000e-05\n","Epoch 62/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.9745 - lr: 1.1220e-05\n","Epoch 63/100\n","93/93 [==============================] - 1s 9ms/step - loss: 19.8018 - lr: 1.2589e-05\n","Epoch 64/100\n","93/93 [==============================] - 1s 10ms/step - loss: 20.0357 - lr: 1.4125e-05\n","Epoch 65/100\n","93/93 [==============================] - 1s 10ms/step - loss: 20.3204 - lr: 1.5849e-05\n","Epoch 66/100\n","93/93 [==============================] - 1s 9ms/step - loss: 21.1362 - lr: 1.7783e-05\n","Epoch 67/100\n","93/93 [==============================] - 1s 9ms/step - loss: 20.4025 - lr: 1.9953e-05\n","Epoch 68/100\n","93/93 [==============================] - 1s 9ms/step - loss: 21.9886 - lr: 2.2387e-05\n","Epoch 69/100\n","93/93 [==============================] - 1s 10ms/step - loss: 21.7997 - lr: 2.5119e-05\n","Epoch 70/100\n","93/93 [==============================] - 1s 9ms/step - loss: 21.5448 - lr: 2.8184e-05\n","Epoch 71/100\n","93/93 [==============================] - 1s 11ms/step - loss: 21.3048 - lr: 3.1623e-05\n","Epoch 72/100\n","93/93 [==============================] - 1s 11ms/step - loss: 20.0929 - lr: 3.5481e-05\n","Epoch 73/100\n","93/93 [==============================] - 1s 11ms/step - loss: 21.3212 - lr: 3.9811e-05\n","Epoch 74/100\n","93/93 [==============================] - 1s 9ms/step - loss: 26.6766 - lr: 4.4668e-05\n","Epoch 75/100\n","93/93 [==============================] - 1s 9ms/step - loss: 28.8565 - lr: 5.0119e-05\n","Epoch 76/100\n","93/93 [==============================] - 1s 10ms/step - loss: 25.2113 - lr: 5.6234e-05\n","Epoch 77/100\n","93/93 [==============================] - 1s 9ms/step - loss: 27.7597 - lr: 6.3096e-05\n","Epoch 78/100\n","93/93 [==============================] - 1s 9ms/step - loss: 26.1925 - lr: 7.0795e-05\n","Epoch 79/100\n","93/93 [==============================] - 1s 10ms/step - loss: 22.9756 - lr: 7.9433e-05\n","Epoch 80/100\n","93/93 [==============================] - 1s 10ms/step - loss: 23.8885 - lr: 8.9125e-05\n","Epoch 81/100\n","93/93 [==============================] - 1s 10ms/step - loss: 23.3846 - lr: 1.0000e-04\n","Epoch 82/100\n","93/93 [==============================] - 1s 9ms/step - loss: 23.1706 - lr: 1.1220e-04\n","Epoch 83/100\n","93/93 [==============================] - 1s 9ms/step - loss: 24.8734 - lr: 1.2589e-04\n","Epoch 84/100\n","93/93 [==============================] - 1s 9ms/step - loss: 26.8818 - lr: 1.4125e-04\n","Epoch 85/100\n","93/93 [==============================] - 1s 9ms/step - loss: 28.4973 - lr: 1.5849e-04\n","Epoch 86/100\n","93/93 [==============================] - 1s 9ms/step - loss: 29.2176 - lr: 1.7783e-04\n","Epoch 87/100\n","93/93 [==============================] - 1s 10ms/step - loss: 34.4473 - lr: 1.9953e-04\n","Epoch 88/100\n","93/93 [==============================] - 1s 9ms/step - loss: 27.9980 - lr: 2.2387e-04\n","Epoch 89/100\n","93/93 [==============================] - 1s 9ms/step - loss: 27.5051 - lr: 2.5119e-04\n","Epoch 90/100\n","93/93 [==============================] - 1s 10ms/step - loss: 42.9334 - lr: 2.8184e-04\n","Epoch 91/100\n","93/93 [==============================] - 1s 9ms/step - loss: 60.8704 - lr: 3.1623e-04\n","Epoch 92/100\n","93/93 [==============================] - 1s 9ms/step - loss: 56.9316 - lr: 3.5481e-04\n","Epoch 93/100\n","93/93 [==============================] - 1s 9ms/step - loss: 61.9596 - lr: 3.9811e-04\n","Epoch 94/100\n","93/93 [==============================] - 1s 9ms/step - loss: 61.0707 - lr: 4.4668e-04\n","Epoch 95/100\n","93/93 [==============================] - 1s 10ms/step - loss: 62.0564 - lr: 5.0119e-04\n","Epoch 96/100\n","93/93 [==============================] - 1s 9ms/step - loss: 65.0926 - lr: 5.6234e-04\n","Epoch 97/100\n","93/93 [==============================] - 1s 9ms/step - loss: 59.3166 - lr: 6.3096e-04\n","Epoch 98/100\n","93/93 [==============================] - 1s 9ms/step - loss: 63.4968 - lr: 7.0795e-04\n","Epoch 99/100\n","93/93 [==============================] - 1s 10ms/step - loss: 67.1191 - lr: 7.9433e-04\n","Epoch 100/100\n","93/93 [==============================] - 1s 9ms/step - loss: 72.3970 - lr: 8.9125e-04\n"]}],"source":["# Set the learning rate scheduler\n","lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n","    lambda epoch: 1e-8 * 10**(epoch / 20))\n","\n","# Initialize the optimizer\n","optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n","\n","# Set the training parameters\n","model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n","\n","# Train the model\n","history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVcKmg7Q_7rD"},"outputs":[],"source":["# Define the learning rate array\n","lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n","\n","# Set the figure size\n","plt.figure(figsize=(10, 6))\n","\n","# Set the grid\n","plt.grid(True)\n","\n","# Plot the loss in log scale\n","plt.semilogx(lrs, history.history[\"loss\"])\n","\n","# Increase the tickmarks size\n","plt.tick_params('both', length=10, width=1, which='both')\n","\n","# Set the plot boundaries\n","plt.axis([1e-8, 1e-3, 0, 100])"]},{"cell_type":"markdown","metadata":{"id":"i-Pn-n60X3uV"},"source":["## Train the Model\n","\n","Now you can proceed to reset and train the model. It is set for 100 epochs in the cell below but feel free to increase it if you want. Laurence got his results in the lectures after 500."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsksvkcXAAgq"},"outputs":[],"source":["# Reset states generated by Keras\n","tf.keras.backend.clear_session()\n","\n","# Reset the weights\n","model.set_weights(init_weights)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"kBYFogmdFM-R","executionInfo":{"status":"ok","timestamp":1673534506218,"user_tz":300,"elapsed":165,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}}},"outputs":[],"source":["# Set the learning rate\n","learning_rate = 8e-7\n","\n","# Set the optimizer \n","optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n","\n","# Set the training parameters\n","model.compile(loss=tf.keras.losses.Huber(),\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"LrcqLyLALmCY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673534643424,"user_tz":300,"elapsed":132474,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"3c72af6e-f898-44fa-ae88-0325efcc774e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","93/93 [==============================] - 13s 14ms/step - loss: 37.4585 - mae: 37.9546\n","Epoch 2/100\n","93/93 [==============================] - 1s 10ms/step - loss: 22.7167 - mae: 23.2118\n","Epoch 3/100\n","93/93 [==============================] - 1s 10ms/step - loss: 20.4971 - mae: 20.9912\n","Epoch 4/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.3825 - mae: 19.8755\n","Epoch 5/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.4397 - mae: 19.9334\n","Epoch 6/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.8020 - mae: 19.2950\n","Epoch 7/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.9478 - mae: 20.4402\n","Epoch 8/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.5911 - mae: 20.0842\n","Epoch 9/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.7247 - mae: 20.2169\n","Epoch 10/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.1374 - mae: 19.6302\n","Epoch 11/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.5843 - mae: 19.0763\n","Epoch 12/100\n","93/93 [==============================] - 1s 11ms/step - loss: 19.0118 - mae: 19.5062\n","Epoch 13/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.0268 - mae: 18.5192\n","Epoch 14/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.6318 - mae: 19.1225\n","Epoch 15/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.1730 - mae: 19.6659\n","Epoch 16/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.1073 - mae: 18.5991\n","Epoch 17/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.7071 - mae: 19.2002\n","Epoch 18/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.3113 - mae: 18.8045\n","Epoch 19/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.9311 - mae: 18.4240\n","Epoch 20/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.3956 - mae: 18.8878\n","Epoch 21/100\n","93/93 [==============================] - 1s 11ms/step - loss: 18.7406 - mae: 19.2331\n","Epoch 22/100\n","93/93 [==============================] - 1s 11ms/step - loss: 18.2405 - mae: 18.7330\n","Epoch 23/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.5638 - mae: 19.0572\n","Epoch 24/100\n","93/93 [==============================] - 1s 10ms/step - loss: 19.5954 - mae: 20.0875\n","Epoch 25/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.5666 - mae: 19.0589\n","Epoch 26/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7447 - mae: 18.2362\n","Epoch 27/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.8195 - mae: 18.3117\n","Epoch 28/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.8445 - mae: 18.3358\n","Epoch 29/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.6881 - mae: 18.1786\n","Epoch 30/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.9877 - mae: 18.4799\n","Epoch 31/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7976 - mae: 18.2881\n","Epoch 32/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.4945 - mae: 18.9873\n","Epoch 33/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.5702 - mae: 18.0621\n","Epoch 34/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.6533 - mae: 18.1457\n","Epoch 35/100\n","93/93 [==============================] - 1s 11ms/step - loss: 18.0954 - mae: 18.5873\n","Epoch 36/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.1111 - mae: 18.6038\n","Epoch 37/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.9337 - mae: 18.4246\n","Epoch 38/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.8713 - mae: 18.3630\n","Epoch 39/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.9241 - mae: 18.4171\n","Epoch 40/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.9251 - mae: 18.4176\n","Epoch 41/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.1292 - mae: 18.6202\n","Epoch 42/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.9141 - mae: 18.4071\n","Epoch 43/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.4037 - mae: 17.8970\n","Epoch 44/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.5795 - mae: 18.0716\n","Epoch 45/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7799 - mae: 18.2731\n","Epoch 46/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.3258 - mae: 18.8175\n","Epoch 47/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.2691 - mae: 18.7610\n","Epoch 48/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.8481 - mae: 18.3401\n","Epoch 49/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.3920 - mae: 17.8835\n","Epoch 50/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.8921 - mae: 18.3835\n","Epoch 51/100\n","93/93 [==============================] - 1s 11ms/step - loss: 18.1588 - mae: 18.6508\n","Epoch 52/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.2342 - mae: 18.7249\n","Epoch 53/100\n","93/93 [==============================] - 1s 11ms/step - loss: 18.0960 - mae: 18.5874\n","Epoch 54/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7184 - mae: 18.2109\n","Epoch 55/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.8326 - mae: 18.3241\n","Epoch 56/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.4034 - mae: 17.8964\n","Epoch 57/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7502 - mae: 18.2414\n","Epoch 58/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.3674 - mae: 17.8588\n","Epoch 59/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.6168 - mae: 18.1088\n","Epoch 60/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.5575 - mae: 18.0492\n","Epoch 61/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.5786 - mae: 18.0704\n","Epoch 62/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.8374 - mae: 18.3287\n","Epoch 63/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.4302 - mae: 17.9203\n","Epoch 64/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.2206 - mae: 17.7112\n","Epoch 65/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.2693 - mae: 17.7600\n","Epoch 66/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.6436 - mae: 18.1342\n","Epoch 67/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.2388 - mae: 17.7285\n","Epoch 68/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.2759 - mae: 17.7639\n","Epoch 69/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.9496 - mae: 18.4404\n","Epoch 70/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.5156 - mae: 18.0069\n","Epoch 71/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.3339 - mae: 17.8247\n","Epoch 72/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.5352 - mae: 18.0281\n","Epoch 73/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.3685 - mae: 17.8590\n","Epoch 74/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7471 - mae: 18.2399\n","Epoch 75/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.5965 - mae: 18.0902\n","Epoch 76/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7764 - mae: 18.2688\n","Epoch 77/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.2626 - mae: 17.7528\n","Epoch 78/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.1036 - mae: 17.5951\n","Epoch 79/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.1936 - mae: 17.6853\n","Epoch 80/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.4271 - mae: 17.9180\n","Epoch 81/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.4143 - mae: 17.9056\n","Epoch 82/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.3070 - mae: 17.7986\n","Epoch 83/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.3311 - mae: 17.8209\n","Epoch 84/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.8889 - mae: 18.3814\n","Epoch 85/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.0203 - mae: 18.5120\n","Epoch 86/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.0893 - mae: 17.5805\n","Epoch 87/100\n","93/93 [==============================] - 1s 11ms/step - loss: 17.2341 - mae: 17.7248\n","Epoch 88/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.2413 - mae: 17.7331\n","Epoch 89/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.0459 - mae: 17.5372\n","Epoch 90/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.4553 - mae: 17.9453\n","Epoch 91/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.5097 - mae: 18.0018\n","Epoch 92/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.0314 - mae: 17.5212\n","Epoch 93/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.7944 - mae: 18.2872\n","Epoch 94/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.0581 - mae: 17.5489\n","Epoch 95/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.4521 - mae: 17.9450\n","Epoch 96/100\n","93/93 [==============================] - 1s 10ms/step - loss: 18.0620 - mae: 18.5538\n","Epoch 97/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.2173 - mae: 17.7098\n","Epoch 98/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.2538 - mae: 17.7444\n","Epoch 99/100\n","93/93 [==============================] - 1s 11ms/step - loss: 16.9978 - mae: 17.4888\n","Epoch 100/100\n","93/93 [==============================] - 1s 10ms/step - loss: 17.8485 - mae: 18.3410\n"]}],"source":["# Train the model\n","history = model.fit(train_set,epochs=100)"]},{"cell_type":"markdown","metadata":{"id":"zAzIkJd0L6WD"},"source":["You can visualize the training and see if the loss and MAE are still trending down."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MD2kyYUVt3O0"},"outputs":[],"source":["# Get mae and loss from history log\n","mae=history.history['mae']\n","loss=history.history['loss']\n","\n","# Get number of epochs\n","epochs=range(len(loss)) \n","\n","# Plot mae and loss\n","plot_series(\n","    x=epochs, \n","    y=(mae, loss), \n","    title='MAE and Loss', \n","    xlabel='MAE',\n","    ylabel='Loss',\n","    legend=['MAE', 'Loss']\n","    )\n","\n","# Only plot the last 80% of the epochs\n","zoom_split = int(epochs[-1] * 0.2)\n","epochs_zoom = epochs[zoom_split:]\n","mae_zoom = mae[zoom_split:]\n","loss_zoom = loss[zoom_split:]\n","\n","# Plot zoomed mae and loss\n","plot_series(\n","    x=epochs_zoom, \n","    y=(mae_zoom, loss_zoom), \n","    title='MAE and Loss', \n","    xlabel='MAE',\n","    ylabel='Loss',\n","    legend=['MAE', 'Loss']\n","    )"]},{"cell_type":"markdown","metadata":{"id":"BHxos0eEX7b7"},"source":["## Model Prediction\n","\n","As before, you can get the predictions for the validation set time range and compute the metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XwGrf-A_wF0"},"outputs":[],"source":["def model_forecast(model, series, window_size, batch_size):\n","    \"\"\"Uses an input model to generate predictions on data windows\n","\n","    Args:\n","      model (TF Keras Model) - model that accepts data windows\n","      series (array of float) - contains the values of the time series\n","      window_size (int) - the number of time steps to include in the window\n","      batch_size (int) - the batch size\n","\n","    Returns:\n","      forecast (numpy array) - array containing predictions\n","    \"\"\"\n","\n","    # Generate a TF Dataset from the series values\n","    dataset = tf.data.Dataset.from_tensor_slices(series)\n","\n","    # Window the data but only take those with the specified size\n","    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n","\n","    # Flatten the windows by putting its elements in a single batch\n","    dataset = dataset.flat_map(lambda w: w.batch(window_size))\n","    \n","    # Create batches of windows\n","    dataset = dataset.batch(batch_size).prefetch(1)\n","    \n","    # Get predictions on the entire dataset\n","    forecast = model.predict(dataset)\n","    \n","    return forecast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5f7kKbFL6zv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673370487821,"user_tz":300,"elapsed":1546,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"676b97b7-bfef-4fe5-95f0-cee8790f4507"},"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 1s 20ms/step\n"]}],"source":["# Reduce the original series\n","forecast_series = series[split_time-window_size:-1]\n","\n","# Use helper function to generate predictions\n","forecast = model_forecast(model, forecast_series, window_size, batch_size)\n","\n","# Drop single dimensional axis\n","results = forecast.squeeze()\n","\n","# Plot the results\n","#plot_series(time_valid, (x_valid, results))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jZrRYjQMNHr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673370489565,"user_tz":300,"elapsed":4,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"8ae5b8a8-d259-4951-909b-9ed8fdf0f79e"},"outputs":[{"output_type":"stream","name":"stdout","text":["16.372328\n"]}],"source":["# Compute the MAE\n","print(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())"]},{"cell_type":"markdown","metadata":{"id":"OXwSQIdQ0rfT"},"source":["## Wrap Up\n","\n","This concludes the final practice lab for this course! You implemented a deep and complex architecture composed of CNNs, RNNs, and DNNs. You'll be using the skills you developed throughout this course to complete the final assignment. Keep it up!"]},{"cell_type":"markdown","metadata":{"id":"qadrbaupYWZp"},"source":["## Optional\n","\n","In this optional section, you will look at another way to dynamically set the learning rate. As you may have noticed, training for a long time generates less and less changes to the loss and metrics. You can run the cell below to observe that again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRSiHiPXg9BQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673370514121,"user_tz":300,"elapsed":18494,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"bf3e8da6-6236-460e-e065-a07bf8a5d05d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","93/93 [==============================] - 7s 14ms/step - loss: 16.4348 - mae: 16.9228\n","Epoch 2/10\n","93/93 [==============================] - 2s 13ms/step - loss: 16.3596 - mae: 16.8494\n","Epoch 3/10\n","93/93 [==============================] - 1s 10ms/step - loss: 16.3944 - mae: 16.8849\n","Epoch 4/10\n","93/93 [==============================] - 1s 10ms/step - loss: 16.3281 - mae: 16.8184\n","Epoch 5/10\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3024 - mae: 16.7919\n","Epoch 6/10\n","93/93 [==============================] - 1s 10ms/step - loss: 16.3640 - mae: 16.8536\n","Epoch 7/10\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3332 - mae: 16.8242\n","Epoch 8/10\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2935 - mae: 16.7818\n","Epoch 9/10\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3149 - mae: 16.8038\n","Epoch 10/10\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2915 - mae: 16.7818\n"]}],"source":["# Initialize the optimizer\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1e-7, momentum=0.9)\n","\n","# Set the training parameters\n","model.compile(loss=tf.keras.losses.Huber(),\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])\n","\n","# Train the model\n","history = model.fit(train_set,epochs=10)"]},{"cell_type":"markdown","metadata":{"id":"JRvZUVW_Nkek"},"source":["At some point, the static learning rate you set might no longer be the optimal one when the model has been learning for some time. You may want to decrease it some more to see better improvements. One way to do that is to have your training loop gradually decay the learning rate per epoch. You can pass in a lambda function similar like the one you did for the learning rate scheduler earlier, or use [ExponentialDecay()](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay). This is a built in scheduler from the Keras API. This decays the learning rate defined by this function:\n","\n","```\n","def decayed_learning_rate(step):\n","  return initial_learning_rate * decay_rate ^ (step / decay_steps)\n","```\n","\n","See how it is used below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1v9hyAmUov66","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673370651564,"user_tz":300,"elapsed":48628,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"6f63ff80-cbb9-47f8-89a9-58aa97d75bab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","93/93 [==============================] - 5s 17ms/step - loss: 16.2986 - mae: 16.7882\n","Epoch 2/40\n","93/93 [==============================] - 1s 14ms/step - loss: 16.3142 - mae: 16.8034\n","Epoch 3/40\n","93/93 [==============================] - 2s 18ms/step - loss: 16.2760 - mae: 16.7663\n","Epoch 4/40\n","93/93 [==============================] - 2s 16ms/step - loss: 16.2904 - mae: 16.7801\n","Epoch 5/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3450 - mae: 16.8347\n","Epoch 6/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2737 - mae: 16.7627\n","Epoch 7/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2838 - mae: 16.7733\n","Epoch 8/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.3068 - mae: 16.7974\n","Epoch 9/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2795 - mae: 16.7687\n","Epoch 10/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3059 - mae: 16.7950\n","Epoch 11/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2507 - mae: 16.7402\n","Epoch 12/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2654 - mae: 16.7554\n","Epoch 13/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3068 - mae: 16.7957\n","Epoch 14/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2601 - mae: 16.7499\n","Epoch 15/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3020 - mae: 16.7911\n","Epoch 16/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2378 - mae: 16.7272\n","Epoch 17/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2104 - mae: 16.7000\n","Epoch 18/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2744 - mae: 16.7632\n","Epoch 19/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2866 - mae: 16.7757\n","Epoch 20/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2800 - mae: 16.7687\n","Epoch 21/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3327 - mae: 16.8228\n","Epoch 22/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.3059 - mae: 16.7962\n","Epoch 23/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2637 - mae: 16.7538\n","Epoch 24/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2659 - mae: 16.7551\n","Epoch 25/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2294 - mae: 16.7178\n","Epoch 26/40\n","93/93 [==============================] - 1s 13ms/step - loss: 16.2346 - mae: 16.7238\n","Epoch 27/40\n","93/93 [==============================] - 2s 16ms/step - loss: 16.2625 - mae: 16.7525\n","Epoch 28/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2401 - mae: 16.7290\n","Epoch 29/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2568 - mae: 16.7462\n","Epoch 30/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2131 - mae: 16.7020\n","Epoch 31/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2624 - mae: 16.7506\n","Epoch 32/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2498 - mae: 16.7384\n","Epoch 33/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2460 - mae: 16.7348\n","Epoch 34/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2181 - mae: 16.7069\n","Epoch 35/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2269 - mae: 16.7157\n","Epoch 36/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2550 - mae: 16.7439\n","Epoch 37/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2233 - mae: 16.7124\n","Epoch 38/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2309 - mae: 16.7204\n","Epoch 39/40\n","93/93 [==============================] - 1s 10ms/step - loss: 16.2342 - mae: 16.7240\n","Epoch 40/40\n","93/93 [==============================] - 1s 9ms/step - loss: 16.2475 - mae: 16.7371\n"]}],"source":["# Set the initial learning rate\n","initial_learning_rate=1e-7\n","\n","# Define the scheduler\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate,\n","    decay_steps=400,\n","    decay_rate=0.96,\n","    staircase=True)\n","\n","# Set the optimizer\n","optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n","\n","# Set the training parameters\n","model.compile(loss=tf.keras.losses.Huber(),\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])\n","\n","# Train the model\n","history = model.fit(train_set,epochs=40)"]},{"cell_type":"markdown","metadata":{"id":"-2OTK8RmPVbZ"},"source":["Now see the results by geting predicitons and computing the metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Syj7stJSQkV0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673370657574,"user_tz":300,"elapsed":879,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"c05c457b-b14d-4219-97ba-6fd65069d3a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["8/8 [==============================] - 1s 8ms/step\n"]}],"source":["# Reduce the original series\n","forecast_series = series[split_time-window_size:-1]\n","\n","# Use helper function to generate predictions\n","forecast = model_forecast(model, forecast_series, window_size, batch_size)\n","\n","# Drop single dimensional axis\n","results = forecast.squeeze()\n","\n","# Plot the results\n","#plot_series(time_valid, (x_valid, results))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPjq8diYQluc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673370660935,"user_tz":300,"elapsed":239,"user":{"displayName":"Ryan Condotta","userId":"17599567598151964519"}},"outputId":"35f885ec-7695-4b1e-fab6-2e30de714afe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["14.47898"]},"metadata":{},"execution_count":20}],"source":["# Compute the MAE\n","tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}